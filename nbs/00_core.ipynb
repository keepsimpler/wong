{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from wong.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "> All the basic functions and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OprtType(Enum):\n",
    "    \"Operator types.`Nothing` means not any operator there.\"\n",
    "    Nothing = 0\n",
    "    Conv2d  = 1\n",
    "    ReLU = 2\n",
    "    BatchNorm2d = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def conv_unit(ni:int, no:int, seq:tuple, ks:int=3, stride:int=1, groups:int=1, zero_bn:bool=False, act_inplace=True):\n",
    "    \"\"\"\n",
    "    The basic convolutional operation, which is combination of operators such as conv, bn, relu, etc.\n",
    "    \"\"\"\n",
    "    unit = []\n",
    "    has_conv = False # if has conv operator\n",
    "    for e in seq:\n",
    "        if e == OprtType.Nothing:  # None operator\n",
    "            continue\n",
    "        elif e == OprtType.Conv2d:  # conv operator\n",
    "            has_conv = True\n",
    "            unit += [nn.Conv2d(ni, no, ks, stride=stride, padding=ks//2, groups=groups, bias=False)]\n",
    "        elif e == OprtType.ReLU:  # relu operator\n",
    "            unit += [nn.ReLU(inplace=act_inplace)]  # in folded resnet, inplace has to be false\n",
    "        elif e == OprtType.BatchNorm2d:  # bn operator\n",
    "            if has_conv: # if has conv operator\n",
    "                bn = nn.BatchNorm2d(no)  # bn operator's `ni` equal to 'no' of conv op\n",
    "                nn.init.constant_(bn.weight, 0. if zero_bn else 1.) # zero bn only after conv\n",
    "                unit += [bn]\n",
    "            else:  # if has not conv operator\n",
    "                unit += [nn.BatchNorm2d(ni)] # bn operator's `ni` equal to 'ni' of conv op\n",
    "    return nn.Sequential(*unit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- `ni` : number of input channels\n",
    "- `no` : number of output channels\n",
    "- `seq` : sequence of operators, a tuple of `OprtType` operator types\n",
    "- `ks` : kernel size of conv operator\n",
    "- `stride` : stride size of conv operator\n",
    "- `groups` : number of groups of conv operator\n",
    "- `zero_bn` : does initialize zero value for weight of batch norm operator\n",
    "- `act_inplace` : does do the activations in-place.\n",
    "\n",
    "Return:\n",
    "- a nn.Sequential object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\"several customized conv units\"\n",
    "relu_conv_bn = partial(conv_unit, seq = (OprtType.ReLU, OprtType.Conv2d, OprtType.BatchNorm2d))  # Relu-->Conv-->BN\n",
    "conv_bn_relu = partial(conv_unit, seq = (OprtType.Conv2d, OprtType.BatchNorm2d, OprtType.ReLU))  # Conv-->BN-->Relu\n",
    "bn_relu_conv = partial(conv_unit, seq = (OprtType.BatchNorm2d, OprtType.ReLU, OprtType.Conv2d))  # BN-->Relu-->Conv\n",
    "relu_conv = partial(conv_unit, seq = (OprtType.ReLU, OprtType.Conv2d, OprtType.Nothing))  # Relu-->Conv\n",
    "conv_bn = partial(conv_unit, seq = (OprtType.Conv2d, OprtType.BatchNorm2d, OprtType.Nothing))  # Conv-->BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def resnet_basicblock(ni, no, nh, stride:int=1):\n",
    "    \"\"\"\n",
    "    Basic Unit in Residual Networks\n",
    "    \n",
    "    Reference:\n",
    "    Deep Residual Learning for Image Recognition:\n",
    "    https://arxiv.org/abs/1512.03385\n",
    "    \"\"\"\n",
    "    return nn.Sequential(*[*relu_conv_bn(ni, nh, stride=stride).children()], \n",
    "                         *[*relu_conv_bn(nh, no).children()])\n",
    "\n",
    "def resnet_bottleneck(ni, no, nh, stride:int=1, groups:int=1, zero_bn=True):\n",
    "    \"\"\"\n",
    "    Bottleneck Unit in Residual Networks\n",
    "    \n",
    "    Reference:\n",
    "    Deep Residual Learning for Image Recognition:\n",
    "    https://arxiv.org/abs/1512.03385\n",
    "    \"\"\"\n",
    "    return nn.Sequential(*[*relu_conv_bn(ni, nh, ks=1).children()],\n",
    "                         *[*relu_conv_bn(nh, nh, stride=stride, groups=groups).children()],\n",
    "                         *[*relu_conv_bn(nh, no, ks=1, zero_bn=zero_bn).children()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def xception(ni:int, no:int, nh:int, ks:int=3, stride:int=1, zero_bn:bool=False):\n",
    "    \"\"\"\n",
    "    Basic unit in xception networks.\n",
    "    \n",
    "    Reference:\n",
    "    Xception: Deep Learning with Depthwise Separable Convolutions:\n",
    "    https://arxiv.org/abs/1610.02357\n",
    "    \"\"\"\n",
    "    return nn.Sequential(*[*relu_conv(ni, nh, ks=ks, stride=stride, groups=ni).children()],\n",
    "                        *[*conv_bn(nh, no, ks=1, zero_bn=zero_bn).children()]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def resnet_stem(ni:int=3, no:int=64, deep_stem:bool=False):\n",
    "    if deep_stem:\n",
    "        stem = nn.Sequential(*[*conv_bn_relu(ni, no, stride=2).children()],  #downsample\n",
    "                             *[*conv_bn_relu(no, no, stride=1).children()],\n",
    "                             *[*conv_bn_relu(no, no, stride=1).children()],\n",
    "                     )\n",
    "    else:\n",
    "        stem = nn.Sequential(nn.Conv2d(ni, no, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "                             nn.BatchNorm2d(no),\n",
    "                             nn.ReLU(inplace=True))\n",
    "    stem = nn.Sequential(*[*stem.children()],\n",
    "                         nn.MaxPool2d(kernel_size=3, stride=2, padding=1))    \n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Sequential(\n",
       "   (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "   (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (2): ReLU(inplace=True)\n",
       "   (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       " ), Sequential(\n",
       "   (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "   (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (2): ReLU()\n",
       "   (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "   (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (5): ReLU()\n",
       "   (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "   (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (8): ReLU()\n",
       "   (9): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       " ))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_stem(), resnet_stem(deep_stem=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class IdentityMapping2(nn.Module):\n",
    "    \"\"\"\n",
    "    Identity Mapping between input and output, four cases:\n",
    "    1.  stride == 1 and ni == no\n",
    "        input == output\n",
    "    2.  stride == 1 and ni != no\n",
    "        1x1 conv and bn\n",
    "    3.  stride == 2 and ni == no\n",
    "        maxpool or avgpool\n",
    "    4.  stride == 2 and ni != no\n",
    "        (maxpool or avgpool) and 1x1 conv and bn\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, ni:int, no:int, stride:int=1, pooling_type:str='max'):\n",
    "        super(IdentityMapping, self).__init__()\n",
    "        assert stride == 1 or stride == 2\n",
    "        assert pooling_type == 'max' or pooling_type == 'avg'\n",
    "        unit = []\n",
    "        if stride == 2:\n",
    "            if pooling_type == 'max':\n",
    "                downsample = nn.MaxPool2d(kernel_size=3, stride=stride, padding=1)\n",
    "            elif pooling_type == 'avg':\n",
    "                downsample = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1)\n",
    "            unit.append(downsample)\n",
    "        if ni != no:\n",
    "            unit += conv_bn(ni, no, ks=1).children()  #, zero_bn=False\n",
    "        self.unit = nn.Sequential(*unit)\n",
    "    def forward(self, x):\n",
    "        out = self.unit(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class IdentityMapping(nn.Module):\n",
    "    \"\"\" Identity mapping of ResNet.        \n",
    "    \"\"\"\n",
    "    def __init__(self, ni:int, no:int, stride:int=1):\n",
    "        super(IdentityMapping, self).__init__()\n",
    "        assert stride == 1 or stride == 2\n",
    "        unit = []\n",
    "        if not (ni == no and stride == 1):\n",
    "            unit += conv_bn(ni, no, ks=1, stride=stride).children()  #, zero_bn=False\n",
    "        self.unit = nn.Sequential(*unit)\n",
    "    def forward(self, x):\n",
    "        out = self.unit(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(IdentityMapping(\n",
       "   (unit): Sequential(\n",
       "     (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "     (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ), IdentityMapping(\n",
       "   (unit): Sequential(\n",
       "     (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ), IdentityMapping(\n",
       "   (unit): Sequential()\n",
       " ), IdentityMapping(\n",
       "   (unit): Sequential(\n",
       "     (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "     (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IdentityMapping(16, 32, stride=2), IdentityMapping(16, 32, stride=1), \\\n",
    "IdentityMapping(16, 16, stride=1), IdentityMapping(16, 16, stride=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Usually work as the final operator for image processing (classification, object detection, etc.)\n",
    "    \n",
    "    Including:\n",
    "    an average pooling op, which downsampling image resolution to 1x1.\n",
    "    a linear op, which perform classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, ni, no):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.adaptivepool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(ni, no)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.adaptivepool(x)  # out tensor (N, ni, 1, 1)\n",
    "        out = out.view(out.size(0), -1)  # out tensor (N, ni)\n",
    "        out = self.fc(out)  # out tensor (N, no)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_cnn(m):\n",
    "    \"copy from https://github.com/fastai/fastai/blob/master/fastai/vision/models/xresnet.py\"\n",
    "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def num_params(net:nn.Module):\n",
    "    \"Number of parameters of a neural network\"\n",
    "    num_params = 0\n",
    "    for name, param in net.named_parameters():\n",
    "        num = torch.prod(torch.tensor(param.size()))\n",
    "        num_params += num\n",
    "        # print(name, param.size(), num)\n",
    "    return num_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
