{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp resnetx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from wong.imports import *\n",
    "from wong.core import *\n",
    "from wong.config import cfg, assert_cfg\n",
    "\n",
    "from torchvision.models.utils import load_state_dict_from_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNetX\n",
    "> a folded resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_pred(l:int, start_id:int, d:int=1):\n",
    "    \"get predecessor layer id.\"\n",
    "    assert l >= 1 and start_id >= d\n",
    "    if l < start_id or d == 1:  # if the current layer index is less than the fold depth, or if fold depth == 1\n",
    "        pred = l - 1\n",
    "    else:\n",
    "        remainder = (l-1-(start_id-d)) % (d-1)\n",
    "        pred = l - 2 * (1+remainder)\n",
    "#         if remainder == 0:\n",
    "#             pred = l - 2 * (d-1)\n",
    "#         else:\n",
    "#             pred = l - 2 * remainder\n",
    "#         remainder1 = l % (2*(d-1))\n",
    "#         if 1 <= remainder1 <= d-1:\n",
    "#             pred = l - 2 * remainder1\n",
    "#         else:\n",
    "#             remainder2 = (remainder1 + d-1) % (2*(d-1))\n",
    "#             pred = l - 2 * remainder2\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- l : current layer id.\n",
    "- start_id : index of the starting node\n",
    "- d : fold depth.\n",
    "\n",
    "Return:\n",
    "- The previous layer id that directly link to the current layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pred(l=50, start_id=8, d=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(get_pred(l=12, start_id=1, d=1), 11)\n",
    "\n",
    "test_eq(get_pred(l=8, start_id=7, d=5), 4)\n",
    "test_eq(get_pred(l=12, start_id=6, d=4), 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def layer_diff(cur:int, pred:int, num_nodes:tuple):\n",
    "    \"layer difference between the current layer and the predecessor layer.\"\n",
    "    assert cur > pred\n",
    "    num_nodes = (1,) + num_nodes\n",
    "    cumsum = 0  # start with 0\n",
    "    for i, num in enumerate(num_nodes):\n",
    "        if cumsum <= cur < cumsum + num:\n",
    "            cur_layer = i\n",
    "        if cumsum <= pred < cumsum + num:\n",
    "            pred_layer = i\n",
    "        cumsum += num\n",
    "    diff = cur_layer - pred_layer\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes = (3,4,6,3)\n",
    "cur, pred = 4,0\n",
    "layer_diff(cur, pred, num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- Stem : the stemming stage, which accept original images, transform them, then input into the backbone network.\n",
    "- Unit : the operation at nodes.\n",
    "- fold : the fold depth\n",
    "- ni : number of input channels of the backbone network.\n",
    "- *num_stages : number of stages in the backbone network.*\n",
    "- num_nodes : number of nodes of every stage in the backbone network.\n",
    "- start_id : index of starting node of ResNetX\n",
    "- base : standard width of channels in the backbone network.\n",
    "- exp : expansion along with the increase of stages.\n",
    "- bottle_scale : bottleneck scale\n",
    "- first_downsample: dose down-sample at the start of the first stage.\n",
    "- deep_stem : using 7x7 or 3 3x3 conv in stemming stage.\n",
    "- c_in : number of input channels of the Start layer\n",
    "- c_out : number of classes in the output of the final classifier.\n",
    "- kwargs : arguments translate into `Unit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResNetX(nn.Module):\n",
    "    \"A folded resnet.\"\n",
    "    def __init__(self, Stem, Unit, fold:int, ni:int, num_nodes:tuple, start_id:int=0, base:int=64, exp:int=2, \n",
    "                 bottle_scale:int=1, first_downsample:bool=False,\n",
    "                 c_in:int=3, c_out:int=10, **kwargs):\n",
    "        super(ResNetX, self).__init__()\n",
    "        # fold depth should be less than the sum length of any two neighboring stages\n",
    "        \n",
    "        if start_id < fold: start_id = fold\n",
    "        origin_ni = ni\n",
    "        num_stages = len(num_nodes)\n",
    "        nhs = [base * exp ** i for i in range(num_stages)] \n",
    "        nos = [nh * bottle_scale for nh in nhs]\n",
    "        strides = [1 if i==0 and not first_downsample else 2 for i in range(num_stages)]\n",
    "#         print('nhs=', nhs, 'nos=', nos, 'nus=', nus, 'strides=', strides)\n",
    "        \n",
    "        self.stem = Stem(c_in, ni) # , deep_stem\n",
    "        \n",
    "        units = []\n",
    "        idmappings = []\n",
    "        cur = 1\n",
    "        for i, (nh, no, nu, stride) in enumerate(zip(nhs, nos, num_nodes, strides)):\n",
    "            for j in range(nu):\n",
    "                if j == 0: # the first node(layer) of each stage\n",
    "                    units += [Unit(ni, no, nh, stride=stride, **kwargs)]\n",
    "                else:\n",
    "                    units += [Unit(no, no, nh, stride=1, **kwargs)]\n",
    "                    \n",
    "                pred = get_pred(cur, start_id, fold) # \n",
    "                diff = layer_diff(cur, pred, num_nodes)\n",
    "                assert diff == 0 or diff == 1 or (diff == 2 and pred == 0), \\\n",
    "                       'cur={}, pred={}, diff={} is not allowed.'.format(cur, pred, diff)\n",
    "#                 print('cur = {} , pred = {} ,diff = {}'.format(cur, pred, diff))\n",
    "                if diff == 0:\n",
    "                    idmappings += [IdentityMapping(no, no, stride=1)]\n",
    "                elif diff == 1:\n",
    "                    idmappings += [IdentityMapping(ni, no, stride=stride)]\n",
    "                elif diff == 2:\n",
    "                    idmappings += [IdentityMapping(origin_ni, no, stride=stride)]\n",
    "                cur += 1\n",
    "            ni = no\n",
    "        self.units = nn.ModuleList(units)\n",
    "        self.idmappings = nn.ModuleList(idmappings)\n",
    "        \n",
    "        self.classifier = Classifier(nos[-1], c_out)\n",
    "        self.start_id, self.fold = start_id, fold\n",
    "        self.num_nodes = num_nodes\n",
    "        init_cnn(self)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        results = {}\n",
    "        results[0] = self.stem(x)\n",
    "        cur = 0\n",
    "        for i, (unit, idmapping) in enumerate(zip(self.units, self.idmappings)):\n",
    "            cur += 1\n",
    "            pred = get_pred(cur, self.start_id, self.fold)\n",
    "            results[cur % (2*self.fold-1)] = unit(results[(cur-1) % (2*self.fold-1)]) + idmapping(results[pred % (2*self.fold-1)])\n",
    "        x = results[cur % (2*self.fold-1)]\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        \n",
    "    def my_load_state_dict(self, state_dict, local_to_pretrained):\n",
    "        error_msgs = []\n",
    "        def load(module, prefix=''):\n",
    "            local_name_params = itertools.chain(module._parameters.items(), module._buffers.items())\n",
    "            local_state = {k: v.data for k, v in local_name_params if v is not None}\n",
    "\n",
    "            new_prefix = local_to_pretrained.get(prefix, 'none')\n",
    "            for name, param in local_state.items():\n",
    "                key = new_prefix + name\n",
    "                if key in state_dict:\n",
    "#                     print(key)\n",
    "                    input_param = state_dict[key]\n",
    "\n",
    "                    if input_param.shape != param.shape:\n",
    "                        # local shape should match the one in checkpoint\n",
    "                        error_msgs.append('size mismatch for {}: copying a param with shape {} from checkpoint, '\n",
    "                                          'the shape in current model is {}.'\n",
    "                                          .format(key, input_param.shape, param.shape))\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        param.copy_(input_param)\n",
    "                    except Exception:\n",
    "                        error_msgs.append('While copying the parameter named \"{}\", '\n",
    "                                          'whose dimensions in the model are {} and '\n",
    "                                          'whose dimensions in the checkpoint are {}.'\n",
    "                                          .format(key, param.size(), input_param.size()))\n",
    "                    \n",
    "            for name, child in module._modules.items():\n",
    "                if child is not None:\n",
    "                    load(child, prefix + name + '.')\n",
    "        load(self)\n",
    "        load = None # break load->load reference cycle\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def resnet_local_to_pretrained(num_nodes, start_id, fold):\n",
    "    local_to_pretrained = {  # mapping from the names of local modules to the names of pretrained modules\n",
    "        'stem.0.': 'conv1.',\n",
    "        'stem.1.': 'bn1.',\n",
    "    }\n",
    "\n",
    "    cumsum = 0\n",
    "    for i, num in enumerate(num_nodes):\n",
    "        for j in range(num):\n",
    "            key = 'units.' + str(cumsum + j) + '.'\n",
    "            value = 'layer' + str(i+1) + '.' + str(j) + '.'\n",
    "            downsample0 = 'layer' + str(i+1) + '.0.' + 'downsample.0.'\n",
    "            downsample1 = 'layer' + str(i+1) + '.0.' + 'downsample.1.'\n",
    "\n",
    "            pred = get_pred(cumsum + j + 1, start_id, fold) # \n",
    "            diff = layer_diff(cumsum + j + 1, pred, num_nodes)\n",
    "            if diff == 1:\n",
    "                idmapping0 = 'idmappings.' + str(cumsum + j) + '.unit.0.'\n",
    "                idmapping1 = 'idmappings.' + str(cumsum + j) + '.unit.1.'\n",
    "#                     print(idmapping0, downsample0)\n",
    "#                     print(idmapping1, downsample1)\n",
    "                local_to_pretrained[idmapping0] = downsample0\n",
    "                local_to_pretrained[idmapping1] = downsample1\n",
    "\n",
    "            for a, b in zip(['1.','2.','4.','5.','7.','8.'], ['conv1.','bn1.','conv2.','bn2.','conv3.','bn3.']):\n",
    "#                     print (key + a, value + b)\n",
    "                local_to_pretrained[key + a] = value + b\n",
    "\n",
    "        cumsum += num\n",
    "    \n",
    "    return local_to_pretrained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def resnetx152(cfg_file:str, fold:int, start_id:int, pretrained:bool=False, **kwargs):\n",
    "    cfg.merge_from_file(cfg_file)\n",
    "    assert_cfg(cfg)\n",
    "    cfg.freeze()\n",
    "    Stem = getattr(sys.modules[__name__], cfg.GRAPH.STEM)\n",
    "    Unit = getattr(sys.modules[__name__], cfg.GRAPH.UNIT)\n",
    "    # start_id >= fold + 1, fold <= 6\n",
    "    model = ResNetX(Stem = Stem, Unit = Unit, fold=fold, ni=cfg.GRAPH.NI, num_nodes=cfg.GRAPH.NUM_NODES, \n",
    "                    start_id=start_id, base=cfg.GRAPH.BASE, exp=cfg.GRAPH.EXP, bottle_scale=cfg.GRAPH.BOTTLE_SCALE,\n",
    "                    first_downsample=cfg.GRAPH.FIRST_DOWNSAMPLE, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(cfg.URL)\n",
    "        local_to_pretrained = resnet_local_to_pretrained(cfg.GRAPH.NUM_NODES,start_id, fold)\n",
    "        model.my_load_state_dict(state_dict, local_to_pretrained)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = 'configs/imagenet/resnet/resnet152.yaml'\n",
    "#model = resnetx152(cfg_file, fold=1, start_id=1, pretrained=True)\n",
    "model = resnetx152(cfg_file, fold=1, start_id=1, pretrained=False, c_out=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip** : Three methods to get `class` or `function` object from its string name:\n",
    "\n",
    "- `getattr(sys.modules[__name__], cfg.GRAPH.STEM)`\n",
    "- `globals()[cfg.GRAPH.STEM]`\n",
    "- `eval(cfg.GRAPH.STEM)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    out = model(x)\n",
    "    out.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'64,894,538'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:,}\".format(num_params(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_resnet152 = resnet152()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torchvision.models.resnet'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__module__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
