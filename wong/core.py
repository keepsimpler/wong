#AUTOGENERATED! DO NOT EDIT! File to edit: dev/00_core.ipynb (unless otherwise specified).

__all__ = ['ShuffleBlock', 'OprtType', 'conv_unit', 'relu_conv_bn', 'conv_bn_relu', 'bn_relu_conv', 'relu_conv',
           'conv_bn', 'relu_conv_bn_shuffle', 'pack_relu_conv_bn', 'pack_bn_relu_conv', 'pack_relu_conv_bn_shuffle',
           'resnet_basicblock', 'resnet_bottleneck', 'preresnet_basicblock', 'preresnet_bottleneck', 'xception',
           'mbconv', 'resnet_stem', 'resnet_stem_deep', 'IdentityMappingMaxPool', 'IdentityMappingAvgPool',
           'IdentityMapping', 'Classifier', 'init_cnn', 'num_params']

#Cell
from .imports import *

#Cell
class ShuffleBlock(nn.Module):
    def __init__(self, groups):
        super(ShuffleBlock, self).__init__()
        self.groups = groups

    def forward(self, x):
        '''Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]'''
        N, C, H, W = x.size()
        g = self.groups
        return x.view(N, g, C//g, H, W).permute(0, 2, 1, 3, 4).reshape(N, C, H, W)

#Cell
from enum import Enum

#Cell
class OprtType(Enum):
    "Operator types.`Nothing` means not any operator there."
    Nothing = 0
    Conv2d  = 1
    ReLU = 2
    BatchNorm2d = 3
    Shuffle = 4

#Cell
def conv_unit(ni:int, seq:tuple, no:int=None, ks:int=3, stride:int=1, groups:int=1, zero_bn:bool=False, act_inplace:bool=True):
    """
    The basic convolutional operation, which is combination of operators such as conv, bn, relu, etc.
    """
    if no is None: no = ni
    unit = []
    has_conv = False # if has conv operator
    for e in seq:
        if e == OprtType.Nothing:  # None operator
            continue
        elif e == OprtType.Conv2d:  # conv operator
            has_conv = True
            unit += [nn.Conv2d(ni, no, ks, stride=stride, padding=ks//2, groups=groups, bias=False)]
        elif e == OprtType.ReLU:  # relu operator
            unit += [nn.ReLU(inplace=act_inplace)]  # in folded resnet, inplace has to be false
        elif e == OprtType.BatchNorm2d:  # bn operator
            if has_conv: # if has conv operator
                bn = nn.BatchNorm2d(no)  # bn operator's `ni` equal to 'no' of conv op
                nn.init.constant_(bn.weight, 0. if zero_bn else 1.) # zero bn only after conv
                unit += [bn]
            else:  # if has not conv operator
                unit += [nn.BatchNorm2d(ni)] # bn operator's `ni` equal to 'ni' of conv op
        elif e == OprtType.Shuffle:  # Shuffle operator
            unit += [ShuffleBlock(groups)]
    return nn.Sequential(*unit)


#Cell
"several customized conv units"
relu_conv_bn = partial(conv_unit, seq = (OprtType.ReLU, OprtType.Conv2d, OprtType.BatchNorm2d))  # Relu-->Conv-->BN
conv_bn_relu = partial(conv_unit, seq = (OprtType.Conv2d, OprtType.BatchNorm2d, OprtType.ReLU))  # Conv-->BN-->Relu
bn_relu_conv = partial(conv_unit, seq = (OprtType.BatchNorm2d, OprtType.ReLU, OprtType.Conv2d))  # BN-->Relu-->Conv
relu_conv = partial(conv_unit, seq = (OprtType.ReLU, OprtType.Conv2d, OprtType.Nothing))  # Relu-->Conv
conv_bn = partial(conv_unit, seq = (OprtType.Conv2d, OprtType.BatchNorm2d, OprtType.Nothing))  # Conv-->BN

relu_conv_bn_shuffle = partial(conv_unit, seq = (OprtType.ReLU, OprtType.Conv2d, OprtType.BatchNorm2d, OprtType.Shuffle))  # Relu-->Conv-->BN-->Shuffle

#Cell
def pack_relu_conv_bn(ni, no, nh, stride:int=1, groups:int=1, zero_bn:bool=True):
    "Packed relu_conv_bn unit"
    return relu_conv_bn(ni, no=no, stride=stride, groups=groups, zero_bn=zero_bn)

def pack_bn_relu_conv(ni, no, nh, stride:int=1, groups:int=1, zero_bn:bool=True):
    """"""
    return bn_relu_conv(ni, no=no, stride=stride, groups=groups, zero_bn=zero_bn)

def pack_relu_conv_bn_shuffle(ni, no, nh, stride:int=1, groups:int=1, zero_bn:bool=True):
    """"""
    return relu_conv_bn_shuffle(ni, no=no, stride=stride, groups=groups, zero_bn=zero_bn)


#Cell
def resnet_basicblock(ni, no, nh, stride:int=1):
    """
    Basic Unit in Residual Networks

    Reference:
    Deep Residual Learning for Image Recognition:
    https://arxiv.org/abs/1512.03385
    """
    return nn.Sequential(*relu_conv_bn(ni, no=nh, stride=stride),
                         *relu_conv_bn(nh, no=no))

def resnet_bottleneck(ni, no, nh, stride:int=1, groups:int=1, zero_bn=True):
    """
    Bottleneck Unit in Residual Networks

    Reference:
    Deep Residual Learning for Image Recognition:
    https://arxiv.org/abs/1512.03385
    """
    return nn.Sequential(*relu_conv_bn(ni, no=nh, ks=1),
                         *relu_conv_bn(nh, no=nh, stride=stride, groups=groups),
                         *relu_conv_bn(nh, no=no, ks=1, zero_bn=zero_bn))

#Cell
# residential block
def preresnet_basicblock(ni, no, nh, stride:int=1):
    """
    Basic Unit in Pre-action Residual Networks, ni == no == nh

    Reference:
    ----------
    Identity Mappings in Deep Residual Networks:
    https://arxiv.org/abs/1603.05027
    """
    return nn.Sequential(*bn_relu_conv(ni, no=nh, stride=stride),
                         *bn_relu_conv(nh, no=no))

def preresnet_bottleneck(ni, no, nh, stride:int=1, groups:int=1, zero_bn=True):
    return nn.Sequential(*bn_relu_conv(ni, no=nh, ks=1),
                         *bn_relu_conv(nh, no=nh, stride=stride, groups=groups),
                         *bn_relu_conv(nh, no=no, ks=1, zero_bn=zero_bn))


#Cell
def xception(ni:int, no:int, nh:int, ks:int=3, stride:int=1, zero_bn:bool=False):
    """
    Basic unit in xception networks.

    Reference:
    Xception: Deep Learning with Depthwise Separable Convolutions:
    https://arxiv.org/abs/1610.02357
    """
    return nn.Sequential(*relu_conv(ni, no=nh, ks=ks, stride=stride, groups=ni),
                         *conv_bn(nh, no=no, ks=1, zero_bn=zero_bn)
                        )

#Cell
def mbconv(ni:int, no:int=None, nh:int=None, ks:int=3, stride:int=1, groups:int=None, zero_bn:bool=False):
    "Mobile Inverted Bottleneck block in MobileNetV2"
    if no is None: no = ni
    if nh is None: nh = ni
    if groups is None: groups = nh
    return nn.Sequential(*conv_bn_relu(ni, no=nh, ks=1, stride=1),
                         *conv_bn_relu(nh, no=nh, ks=ks, stride=stride, groups=groups),
                         *conv_bn(nh, no=no, ks=1, stride=1, zero_bn=zero_bn))

#Cell
def resnet_stem(ni:int=3, no:int=64):
    stem = nn.Sequential(nn.Conv2d(ni, no, kernel_size=7, stride=2, padding=3, bias=False),
                         nn.BatchNorm2d(no),
                         nn.ReLU(inplace=True),
                         nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
    return stem

def resnet_stem_deep(ni:int=3, no:int=64):
    stem = nn.Sequential(*conv_bn_relu(ni, no=no, stride=2),  #downsample
                         *conv_bn_relu(no, no=no, stride=1),
                         *conv_bn_relu(no, no=no, stride=1),
                         nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
                 )
    return stem



#Cell
class IdentityMappingMaxPool(nn.Module):
    """
    Identity Mapping using maxpool to accross stage, four cases:
    1.  stride == 1 and ni == no
        input == output
    2.  stride == 1 and ni != no
        1x1 conv --> bn
    3.  stride == 2 and ni == no
        maxpool
    4.  stride == 2 and ni != no
        (maxpool) --> 1x1 conv --> bn

    """
    def __init__(self, ni:int, no:int, stride:int=1):
        super(IdentityMappingMaxPool, self).__init__()
        assert stride == 1 or stride == 2
        unit = []
        if stride == 2:
            downsample = nn.MaxPool2d(kernel_size=3, stride=stride, padding=1)
            unit.append(downsample)
        if ni != no:
            unit += conv_bn(ni, no=no, ks=1) #.children()  #, zero_bn=False
        self.unit = nn.Sequential(*unit)
    def forward(self, x):
        out = self.unit(x)
        return out

class IdentityMappingAvgPool(nn.Module):
    """
    Identity Mapping using avgpool to accross stage, four cases:
    1.  stride == 1 and ni == no
        input == output
    2.  stride == 1 and ni != no
        1x1 conv --> bn
    3.  stride == 2 and ni == no
        avgpool
    4.  stride == 2 and ni != no
        (avgpool) --> 1x1 conv --> bn

    """
    def __init__(self, ni:int, no:int, stride:int=1):
        super(IdentityMappingAvgPool, self).__init__()
        assert stride == 1 or stride == 2
        unit = []
        if stride == 2:
            downsample = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1)
            unit.append(downsample)
        if ni != no:
            unit += conv_bn(ni, no=no, ks=1) #.children()  #, zero_bn=False
        self.unit = nn.Sequential(*unit)
    def forward(self, x):
        out = self.unit(x)
        return out

#Cell
class IdentityMapping(nn.Module):
    """ Identity mapping of ResNet.
    Identity mapping of ResNet, two cases:
    1.  stride == 1 and ni == no
        input == output
    2.  else
        1x1 conv with stride --> bn

    """
    def __init__(self, ni:int, no:int, stride:int=1):
        super(IdentityMapping, self).__init__()
        assert stride == 1 or stride == 2
        unit = []
        if not (ni == no and stride == 1):
            unit += conv_bn(ni, no=no, ks=1, stride=stride) #.children()  #, zero_bn=False
        self.unit = nn.Sequential(*unit)
    def forward(self, x):
        out = self.unit(x)
        return out

#Cell
class Classifier(nn.Module):
    """
    Usually work as the final operator for image processing (classification, object detection, etc.)

    Including:
    an average pooling op, which downsampling image resolution to 1x1.
    a linear op, which perform classification.
    """
    def __init__(self, ni, no):
        super(Classifier, self).__init__()
        self.adaptivepool = nn.AdaptiveAvgPool2d((1,1))
        self.fc = nn.Linear(ni, no)

    def forward(self, x):
        out = self.adaptivepool(x)  # out tensor (N, ni, 1, 1)
        out = out.view(out.size(0), -1)  # out tensor (N, ni)
        out = self.fc(out)  # out tensor (N, no)
        return out

#Cell
def init_cnn(m):
    "copy from https://github.com/fastai/fastai/blob/master/fastai/vision/models/xresnet.py"
    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)
    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)
    for l in m.children(): init_cnn(l)


#Cell
def num_params(net:nn.Module):
    "Number of parameters of a neural network"
    num_params = 0
    for name, param in net.named_parameters():
        num = torch.prod(torch.tensor(param.size()))
        num_params += num
        # print(name, param.size(), num)
    return num_params